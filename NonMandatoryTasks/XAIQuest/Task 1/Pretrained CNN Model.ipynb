{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13426425,"sourceType":"datasetVersion","datasetId":8521872}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader  # ✅ fixed here\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torchvision.models as models\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:48.164510Z","iopub.execute_input":"2025-10-20T13:49:48.164947Z","iopub.status.idle":"2025-10-20T13:49:48.170725Z","shell.execute_reply.started":"2025-10-20T13:49:48.164919Z","shell.execute_reply":"2025-10-20T13:49:48.170058Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import pickle\nBASE_PATH = \"/kaggle/input/cifar-10-and-cifar-10-c/cifar-10-python/cifar-10-batches-py\"\n\ntrain_data = []\ntrain_label = []\n\ndef unpickle(file):\n    with open(file ,'rb') as f:\n        data_dict = pickle.load(f , encoding = 'bytes')\n    return data_dict\n        \nfor i in range(1,6):\n    batch_path = os.path.join(BASE_PATH , f\"data_batch_{i}\" )\n    batch = unpickle(batch_path)\n    train_data.append(batch[b'data'])\n    train_label.extend(batch[b'labels'])\n\nX_train = np.vstack(train_data)      \ny_train = np.array(train_label)     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:48.171943Z","iopub.execute_input":"2025-10-20T13:49:48.172218Z","iopub.status.idle":"2025-10-20T13:49:48.424240Z","shell.execute_reply.started":"2025-10-20T13:49:48.172197Z","shell.execute_reply":"2025-10-20T13:49:48.423633Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"test_data = []\ntest_label = []\n\ntest_path = os.path.join(BASE_PATH , 'test_batch')\ntest_batch = unpickle(test_path)\ntest_data.append(batch[b'data'])\ntest_label.extend(batch[b'labels'])\n\ny_test = np.array(test_label)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:48.425372Z","iopub.execute_input":"2025-10-20T13:49:48.425634Z","iopub.status.idle":"2025-10-20T13:49:48.448044Z","shell.execute_reply.started":"2025-10-20T13:49:48.425618Z","shell.execute_reply":"2025-10-20T13:49:48.447404Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\nCIFAR_STD  = (0.2470, 0.2435, 0.2616)\n\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),         \n    transforms.RandomHorizontalFlip(p=0.5),        \n    transforms.ColorJitter(brightness=0.1, contrast=0.1,\n                           saturation=0.1, hue=0.1),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:48.448662Z","iopub.execute_input":"2025-10-20T13:49:48.448878Z","iopub.status.idle":"2025-10-20T13:49:48.454116Z","shell.execute_reply.started":"2025-10-20T13:49:48.448854Z","shell.execute_reply":"2025-10-20T13:49:48.453505Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def cifar_flat_to_pil(img_flat):\n    r = img_flat[0:1024].reshape(32,32)\n    g = img_flat[1024:2048].reshape(32,32)\n    b = img_flat[2048:3072].reshape(32,32)\n    img = np.stack([r,g,b], axis=2).astype('uint8')\n    pil_img = Image.fromarray(img).convert(\"RGB\")\n    return pil_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:48.455826Z","iopub.execute_input":"2025-10-20T13:49:48.456625Z","iopub.status.idle":"2025-10-20T13:49:48.471843Z","shell.execute_reply.started":"2025-10-20T13:49:48.456607Z","shell.execute_reply":"2025-10-20T13:49:48.471295Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from PIL import Image\ndef cifar_dataset_to_pil(flat_images):\n    pil_images = []\n    for img_flat in flat_images:\n        pil_img = cifar_flat_to_pil(img_flat)  # pass the array directly\n        pil_images.append(pil_img)\n    return pil_images\n\ntrain_data_flat = np.vstack(train_data)\ntest_data_flat = np.vstack(test_data)   \n\ntrain_images = cifar_dataset_to_pil(train_data_flat)\ntest_images = cifar_dataset_to_pil(test_data_flat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:48.472409Z","iopub.execute_input":"2025-10-20T13:49:48.472585Z","iopub.status.idle":"2025-10-20T13:49:50.278889Z","shell.execute_reply.started":"2025-10-20T13:49:48.472571Z","shell.execute_reply":"2025-10-20T13:49:50.278007Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport multiprocessing\n\nclass CIFAR10_dataset(Dataset):\n    def __init__(self, images, labels=None, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n        self.has_labels = labels is not None\n        print(f\"\\nLoading CIFAR10 Dataset... Total Len: {len(images)} | Labeled: {self.has_labels}\\n{'-'*50}\")\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n        if self.transform:\n            img = self.transform(img)\n        if self.has_labels:\n            label = torch.tensor(self.labels[idx], dtype=torch.long)\n            return {\"img\": img, \"label\": label}\n        else:\n            return {\"img\": img}\n\n\ntrain_images_split, val_images, y_train_split, y_val = train_test_split(\n    train_images, y_train,\n    test_size=0.2,         \n    random_state=42,       \n    stratify=y_train        \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:50.279950Z","iopub.execute_input":"2025-10-20T13:49:50.280201Z","iopub.status.idle":"2025-10-20T13:49:50.313592Z","shell.execute_reply.started":"2025-10-20T13:49:50.280175Z","shell.execute_reply":"2025-10-20T13:49:50.313063Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_dataset = CIFAR10_dataset(train_images_split, y_train_split, transform=train_transform)\nval_dataset = CIFAR10_dataset(val_images, y_val, transform=test_transform)\ntest_dataset = CIFAR10_dataset(test_images, labels=None, transform=test_transform)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    shuffle=True,\n    num_workers=multiprocessing.cpu_count(),\n    pin_memory=True,\n    persistent_workers=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=128,\n    shuffle=False,\n    num_workers=multiprocessing.cpu_count(),\n    pin_memory=True,\n    persistent_workers=True\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=128,\n    shuffle=False,\n    num_workers=multiprocessing.cpu_count(),\n    pin_memory=True,\n    persistent_workers=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:50.314260Z","iopub.execute_input":"2025-10-20T13:49:50.314527Z","iopub.status.idle":"2025-10-20T13:49:50.396492Z","shell.execute_reply.started":"2025-10-20T13:49:50.314490Z","shell.execute_reply":"2025-10-20T13:49:50.395707Z"}},"outputs":[{"name":"stdout","text":"\nLoading CIFAR10 Dataset... Total Len: 40000 | Labeled: True\n--------------------------------------------------\n\nLoading CIFAR10 Dataset... Total Len: 10000 | Labeled: True\n--------------------------------------------------\n\nLoading CIFAR10 Dataset... Total Len: 10000 | Labeled: False\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def create_model(model_name, num_classes=10):\n    if model_name == 'ResNet18':\n        model = models.resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif model_name == 'EfficientNet-B0':\n        model = models.efficientnet_b0(pretrained=True)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    elif model_name == 'VGG16':\n        model = models.vgg16(pretrained=True)\n        model.classifier[6] = nn.Linear(4096, num_classes)\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:50.397304Z","iopub.execute_input":"2025-10-20T13:49:50.397566Z","iopub.status.idle":"2025-10-20T13:49:50.419593Z","shell.execute_reply.started":"2025-10-20T13:49:50.397550Z","shell.execute_reply":"2025-10-20T13:49:50.418909Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model_configs = {\n    'ResNet18': {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9},\n    'EfficientNet-B0': {'optimizer': 'Adam', 'lr': 0.001},\n    'VGG16': {'optimizer': 'Adam', 'lr': 0.001}}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:49:50.420275Z","iopub.execute_input":"2025-10-20T13:49:50.420498Z","iopub.status.idle":"2025-10-20T13:49:50.435747Z","shell.execute_reply.started":"2025-10-20T13:49:50.420483Z","shell.execute_reply":"2025-10-20T13:49:50.434999Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\n\n# Create results directory\nos.makedirs('/kaggle/working/model_results', exist_ok=True)\n\n# ==================== MODEL DEFINITIONS ====================\ndef create_model(model_name, num_classes=10):\n    if model_name == 'ResNet18':\n        model = models.resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif model_name == 'EfficientNet-B0':\n        model = models.efficientnet_b0(pretrained=True)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    elif model_name == 'VGG16':\n        model = models.vgg16(pretrained=True)\n        model.classifier[6] = nn.Linear(4096, num_classes)\n    \n    return model\n\n# Models to train\nmodel_configs = {\n    'ResNet18': {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9},\n    'EfficientNet-B0': {'optimizer': 'Adam', 'lr': 0.001},\n    'VGG16': {'optimizer': 'Adam', 'lr': 0.0005}\n}\n\n# ==================== TRAINING FUNCTION ====================\ndef train_model(model, model_name, train_loader, val_loader, epochs=50):\n    print(f\"Training {model_name}\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    config = model_configs[model_name]\n    if config['optimizer'] == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], \n                                  weight_decay=1e-4, momentum=config.get('momentum', 0.9))\n    else:\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], \n                                   weight_decay=1e-4, betas=(0.9, 0.999))\n    \n    criterion = nn.CrossEntropyLoss()\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, min_lr=0.00001)\n    \n    history = {\n        'train_loss': [], 'val_loss': [],\n        'train_acc': [], 'val_acc': [],\n        'val_precision': [], 'val_recall': [], 'val_f1': []\n    }\n    \n    best_val_acc = 0\n    best_model_path = f\"/kaggle/working/model_results/best_{model_name.replace('-', '_')}.pth\"\n    \n    for epoch in range(epochs):\n        \n        model.train()\n        train_loss, train_correct = 0, 0\n        all_train_preds, all_train_labels = [], []\n        \n        for batch in tqdm(train_loader, desc=f'{model_name} - Epoch {epoch+1}/{epochs} [Train]'):\n            images, labels = batch[\"img\"].to(device), batch[\"label\"].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * images.size(0)\n            preds = outputs.argmax(1)\n            train_correct += (preds == labels).sum().item()\n            \n            all_train_preds.extend(preds.cpu().numpy())\n            all_train_labels.extend(labels.cpu().numpy())\n        \n        train_loss = train_loss / len(train_loader.dataset)\n        train_acc = 100. * train_correct / len(train_loader.dataset)\n        \n        # Validation\n        model.eval()\n        val_loss, val_correct = 0, 0\n        all_val_preds, all_val_labels, all_val_probs = [], [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f'{model_name} - Epoch {epoch+1}/{epochs} [Val]'):\n                images, labels = batch[\"img\"].to(device), batch[\"label\"].to(device)\n                \n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item() * images.size(0)\n                preds = outputs.argmax(1)\n                val_correct += (preds == labels).sum().item()\n                \n                probs = torch.softmax(outputs, dim=1)\n                \n                all_val_preds.extend(preds.cpu().numpy())\n                all_val_labels.extend(labels.cpu().numpy())\n                all_val_probs.extend(probs.cpu().numpy())\n        \n        val_loss = val_loss / len(val_loader.dataset)\n        val_acc = 100. * val_correct / len(val_loader.dataset)\n        \n        # Compute metrics\n        from sklearn.metrics import precision_score, recall_score, f1_score\n        \n        val_precision = precision_score(all_val_labels, all_val_preds, average='macro', zero_division=0)\n        val_recall = recall_score(all_val_labels, all_val_preds, average='macro', zero_division=0)\n        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro', zero_division=0)\n        \n        # Store history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n        history['val_precision'].append(val_precision)\n        history['val_recall'].append(val_recall)\n        history['val_f1'].append(val_f1)\n        \n        print(f\"\\n{model_name} - Epoch {epoch+1} Results:\")\n        print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"   Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n        print(f\"   Val Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f}\")\n        print(f\"   LR: {optimizer.param_groups[0]['lr']:.6f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"NEW BEST MODEL! Val Acc: {val_acc:.2f}%\")\n            \n            # Create model-specific directory\n            model_dir = f\"/kaggle/working/model_results/{model_name}\"\n            os.makedirs(model_dir, exist_ok=True)\n            \n            # Save classification report\n            class_names = ['airplane', 'auto', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n            report = classification_report(all_val_labels, all_val_preds, target_names=class_names, output_dict=True)\n            report_df = pd.DataFrame(report).transpose()\n            report_df.to_csv(f'{model_dir}/{model_name}_classification_report.csv')\n            \n            # Save confusion matrix\n            cm = confusion_matrix(all_val_labels, all_val_preds)\n            np.savetxt(f'{model_dir}/{model_name}_confusion_matrix.csv', cm, delimiter=',', fmt='%d')\n            \n            # Save training plots for this model\n            plt.figure(figsize=(15, 5))\n            \n            plt.subplot(1, 3, 1)\n            plt.plot(history['train_loss'][:epoch+1], label='Train Loss', linewidth=2)\n            plt.plot(history['val_loss'][:epoch+1], label='Val Loss', linewidth=2)\n            plt.title(f'{model_name} - Loss')\n            plt.xlabel('Epoch')\n            plt.ylabel('Loss')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            \n            plt.subplot(1, 3, 2)\n            plt.plot(history['train_acc'][:epoch+1], label='Train Acc', linewidth=2)\n            plt.plot(history['val_acc'][:epoch+1], label='Val Acc', linewidth=2)\n            plt.title(f'{model_name} - Accuracy')\n            plt.xlabel('Epoch')\n            plt.ylabel('Accuracy (%)')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            \n            plt.subplot(1, 3, 3)\n            plt.plot(history['val_f1'][:epoch+1], label='Val F1', color='red', linewidth=2)\n            plt.title(f'{model_name} - F1 Score')\n            plt.xlabel('Epoch')\n            plt.ylabel('F1 Score')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            \n            plt.tight_layout()\n            plt.savefig(f'{model_dir}/{model_name}_training_plots.png', dpi=120, bbox_inches='tight')\n            plt.close()\n        \n        lr_scheduler.step(val_loss)\n    \n    print(f\"\\n{model_name} Training Complete! Best Val Acc: {best_val_acc:.2f}%\")\n    \n    return history, best_val_acc\n\n# ==================== TRAIN ALL MODELS ====================\nall_results = {}\n\nfor model_name in model_configs.keys():\n    model = create_model(model_name)\n    history, best_acc = train_model(model, model_name, train_loader, val_loader, epochs=50)\n    all_results[model_name] = {\n        'history': history,\n        'best_val_acc': best_acc\n    }\n    \n    # Save final training history for each model\n    model_dir = f\"/kaggle/working/model_results/{model_name}\"\n    history_df = pd.DataFrame(history)\n    history_df.to_csv(f'{model_dir}/{model_name}_training_history.csv', index=False)\n    print(f\"{model_name} training history saved to {model_dir}\")\n\n# ==================== FINAL SUMMARY ====================\nprint(\"TRAINING COMPLETE - SUMMARY\")\n\nfor model_name, result in all_results.items():\n    best_acc = result['best_val_acc']\n    best_f1 = max(result['history']['val_f1'])\n    print(f\"{model_name}:\")\n    print(f\"   Best Validation Accuracy: {best_acc:.2f}%\")\n    print(f\"   Best F1 Score: {best_f1:.4f}\")\n    print(f\"   Results saved to: /kaggle/working/model_results/{model_name}/\")\n    print()\n\nprint(\"All model results saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:51:05.633928Z","iopub.execute_input":"2025-10-20T13:51:05.634189Z","iopub.status.idle":"2025-10-20T15:05:39.529223Z","shell.execute_reply.started":"2025-10-20T13:51:05.634172Z","shell.execute_reply":"2025-10-20T15:05:39.528294Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 178MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Training ResNet18\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 1/50 [Train]: 100%|██████████| 313/313 [00:22<00:00, 14.23it/s]\nResNet18 - Epoch 1/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 61.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 1 Results:\n   Train Loss: 1.3326 | Train Acc: 53.36%\n   Val Loss: 1.0739 | Val Acc: 62.18%\n   Val Precision: 0.6892 | Recall: 0.6218 | F1: 0.6177\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 62.18%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 2/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.77it/s]\nResNet18 - Epoch 2/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 2 Results:\n   Train Loss: 0.9781 | Train Acc: 66.14%\n   Val Loss: 0.7860 | Val Acc: 72.69%\n   Val Precision: 0.7447 | Recall: 0.7269 | F1: 0.7283\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 72.69%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 3/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.03it/s]\nResNet18 - Epoch 3/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 3 Results:\n   Train Loss: 0.8786 | Train Acc: 70.08%\n   Val Loss: 0.6868 | Val Acc: 76.36%\n   Val Precision: 0.7713 | Recall: 0.7636 | F1: 0.7652\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 76.36%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 4/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.00it/s]\nResNet18 - Epoch 4/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 76.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 4 Results:\n   Train Loss: 0.8200 | Train Acc: 71.59%\n   Val Loss: 0.6782 | Val Acc: 77.11%\n   Val Precision: 0.7810 | Recall: 0.7711 | F1: 0.7717\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 77.11%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 5/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.88it/s]\nResNet18 - Epoch 5/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 5 Results:\n   Train Loss: 0.7597 | Train Acc: 73.65%\n   Val Loss: 0.6281 | Val Acc: 78.27%\n   Val Precision: 0.7884 | Recall: 0.7827 | F1: 0.7820\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 78.27%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 6/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.01it/s]\nResNet18 - Epoch 6/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 78.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 6 Results:\n   Train Loss: 0.7258 | Train Acc: 74.91%\n   Val Loss: 0.5852 | Val Acc: 80.14%\n   Val Precision: 0.8041 | Recall: 0.8014 | F1: 0.8015\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 80.14%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 7/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.22it/s]\nResNet18 - Epoch 7/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 76.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 7 Results:\n   Train Loss: 0.6987 | Train Acc: 75.62%\n   Val Loss: 0.5773 | Val Acc: 79.96%\n   Val Precision: 0.8039 | Recall: 0.7996 | F1: 0.7998\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 8/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.00it/s]\nResNet18 - Epoch 8/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 76.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 8 Results:\n   Train Loss: 0.6697 | Train Acc: 76.67%\n   Val Loss: 0.5832 | Val Acc: 80.23%\n   Val Precision: 0.8058 | Recall: 0.8023 | F1: 0.8026\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 80.23%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 9/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.08it/s]\nResNet18 - Epoch 9/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 78.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 9 Results:\n   Train Loss: 0.6520 | Train Acc: 77.22%\n   Val Loss: 0.5742 | Val Acc: 80.85%\n   Val Precision: 0.8123 | Recall: 0.8085 | F1: 0.8090\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 80.85%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 10/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.01it/s]\nResNet18 - Epoch 10/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 80.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 10 Results:\n   Train Loss: 0.6264 | Train Acc: 78.19%\n   Val Loss: 0.5582 | Val Acc: 80.99%\n   Val Precision: 0.8166 | Recall: 0.8099 | F1: 0.8114\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 80.99%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 11/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.19it/s]\nResNet18 - Epoch 11/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 75.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 11 Results:\n   Train Loss: 0.6161 | Train Acc: 78.39%\n   Val Loss: 0.5615 | Val Acc: 80.79%\n   Val Precision: 0.8166 | Recall: 0.8079 | F1: 0.8098\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 12/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.02it/s]\nResNet18 - Epoch 12/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 76.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 12 Results:\n   Train Loss: 0.5944 | Train Acc: 79.18%\n   Val Loss: 0.6332 | Val Acc: 79.47%\n   Val Precision: 0.8014 | Recall: 0.7947 | F1: 0.7948\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 13/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.93it/s]\nResNet18 - Epoch 13/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 13 Results:\n   Train Loss: 0.5812 | Train Acc: 79.68%\n   Val Loss: 0.5543 | Val Acc: 81.29%\n   Val Precision: 0.8151 | Recall: 0.8129 | F1: 0.8128\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 81.29%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 14/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.06it/s]\nResNet18 - Epoch 14/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 14 Results:\n   Train Loss: 0.5682 | Train Acc: 80.15%\n   Val Loss: 0.5113 | Val Acc: 82.61%\n   Val Precision: 0.8285 | Recall: 0.8261 | F1: 0.8261\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 82.61%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 15/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.99it/s]\nResNet18 - Epoch 15/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 15 Results:\n   Train Loss: 0.5571 | Train Acc: 80.45%\n   Val Loss: 0.5591 | Val Acc: 81.43%\n   Val Precision: 0.8230 | Recall: 0.8143 | F1: 0.8160\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 16/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.93it/s]\nResNet18 - Epoch 16/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 16 Results:\n   Train Loss: 0.5359 | Train Acc: 81.33%\n   Val Loss: 0.5297 | Val Acc: 82.22%\n   Val Precision: 0.8234 | Recall: 0.8222 | F1: 0.8212\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 17/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.98it/s]\nResNet18 - Epoch 17/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 17 Results:\n   Train Loss: 0.5258 | Train Acc: 81.61%\n   Val Loss: 0.5330 | Val Acc: 81.85%\n   Val Precision: 0.8207 | Recall: 0.8185 | F1: 0.8178\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 18/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.77it/s]\nResNet18 - Epoch 18/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 18 Results:\n   Train Loss: 0.5155 | Train Acc: 81.71%\n   Val Loss: 0.5169 | Val Acc: 82.64%\n   Val Precision: 0.8365 | Recall: 0.8264 | F1: 0.8280\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 82.64%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 19/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.82it/s]\nResNet18 - Epoch 19/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 19 Results:\n   Train Loss: 0.5111 | Train Acc: 82.12%\n   Val Loss: 0.6229 | Val Acc: 82.17%\n   Val Precision: 0.8267 | Recall: 0.8217 | F1: 0.8229\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 20/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.91it/s]\nResNet18 - Epoch 20/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 20 Results:\n   Train Loss: 0.4926 | Train Acc: 82.69%\n   Val Loss: 0.5075 | Val Acc: 82.90%\n   Val Precision: 0.8365 | Recall: 0.8290 | F1: 0.8311\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 82.90%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 21/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.93it/s]\nResNet18 - Epoch 21/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 75.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 21 Results:\n   Train Loss: 0.4857 | Train Acc: 82.77%\n   Val Loss: 0.5053 | Val Acc: 83.47%\n   Val Precision: 0.8400 | Recall: 0.8347 | F1: 0.8361\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 83.47%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 22/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.07it/s]\nResNet18 - Epoch 22/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 22 Results:\n   Train Loss: 0.4762 | Train Acc: 83.21%\n   Val Loss: 0.5427 | Val Acc: 82.20%\n   Val Precision: 0.8265 | Recall: 0.8220 | F1: 0.8221\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 23/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.24it/s]\nResNet18 - Epoch 23/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 23 Results:\n   Train Loss: 0.4729 | Train Acc: 83.36%\n   Val Loss: 0.5395 | Val Acc: 82.85%\n   Val Precision: 0.8340 | Recall: 0.8285 | F1: 0.8294\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 24/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.66it/s]\nResNet18 - Epoch 24/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 75.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 24 Results:\n   Train Loss: 0.4654 | Train Acc: 83.47%\n   Val Loss: 0.5211 | Val Acc: 82.81%\n   Val Precision: 0.8325 | Recall: 0.8281 | F1: 0.8291\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 25/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.75it/s]\nResNet18 - Epoch 25/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 78.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 25 Results:\n   Train Loss: 0.4488 | Train Acc: 84.22%\n   Val Loss: 0.5149 | Val Acc: 83.21%\n   Val Precision: 0.8350 | Recall: 0.8321 | F1: 0.8330\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 26/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.31it/s]\nResNet18 - Epoch 26/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 26 Results:\n   Train Loss: 0.4467 | Train Acc: 84.25%\n   Val Loss: 0.5193 | Val Acc: 82.90%\n   Val Precision: 0.8312 | Recall: 0.8290 | F1: 0.8290\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 27/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.26it/s]\nResNet18 - Epoch 27/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 78.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 27 Results:\n   Train Loss: 0.4348 | Train Acc: 84.64%\n   Val Loss: 0.5355 | Val Acc: 82.54%\n   Val Precision: 0.8356 | Recall: 0.8254 | F1: 0.8283\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 28/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.95it/s]\nResNet18 - Epoch 28/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 75.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 28 Results:\n   Train Loss: 0.4245 | Train Acc: 84.94%\n   Val Loss: 0.6182 | Val Acc: 81.94%\n   Val Precision: 0.8256 | Recall: 0.8194 | F1: 0.8205\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 29/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.14it/s]\nResNet18 - Epoch 29/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 29 Results:\n   Train Loss: 0.4278 | Train Acc: 84.86%\n   Val Loss: 0.5156 | Val Acc: 83.39%\n   Val Precision: 0.8343 | Recall: 0.8339 | F1: 0.8329\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 30/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.32it/s]\nResNet18 - Epoch 30/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 30 Results:\n   Train Loss: 0.4167 | Train Acc: 85.10%\n   Val Loss: 0.5065 | Val Acc: 83.89%\n   Val Precision: 0.8405 | Recall: 0.8389 | F1: 0.8391\n   LR: 0.010000\nNEW BEST MODEL! Val Acc: 83.89%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 31/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.42it/s]\nResNet18 - Epoch 31/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 31 Results:\n   Train Loss: 0.4130 | Train Acc: 85.25%\n   Val Loss: 0.5339 | Val Acc: 82.98%\n   Val Precision: 0.8366 | Recall: 0.8298 | F1: 0.8301\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 32/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.33it/s]\nResNet18 - Epoch 32/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 32 Results:\n   Train Loss: 0.4086 | Train Acc: 85.45%\n   Val Loss: 0.5339 | Val Acc: 83.16%\n   Val Precision: 0.8390 | Recall: 0.8316 | F1: 0.8340\n   LR: 0.010000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 33/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.17it/s]\nResNet18 - Epoch 33/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 79.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 33 Results:\n   Train Loss: 0.3425 | Train Acc: 87.75%\n   Val Loss: 0.4657 | Val Acc: 85.18%\n   Val Precision: 0.8550 | Recall: 0.8518 | F1: 0.8530\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.18%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 34/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.47it/s]\nResNet18 - Epoch 34/50 [Val]: 100%|██████████| 79/79 [00:00<00:00, 82.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 34 Results:\n   Train Loss: 0.3114 | Train Acc: 88.81%\n   Val Loss: 0.4677 | Val Acc: 85.53%\n   Val Precision: 0.8563 | Recall: 0.8553 | F1: 0.8554\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.53%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 35/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 15.82it/s]\nResNet18 - Epoch 35/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 72.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 35 Results:\n   Train Loss: 0.2981 | Train Acc: 89.44%\n   Val Loss: 0.4687 | Val Acc: 85.25%\n   Val Precision: 0.8548 | Recall: 0.8525 | F1: 0.8532\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 36/50 [Train]: 100%|██████████| 313/313 [00:19<00:00, 16.28it/s]\nResNet18 - Epoch 36/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 78.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 36 Results:\n   Train Loss: 0.2969 | Train Acc: 89.34%\n   Val Loss: 0.4672 | Val Acc: 85.73%\n   Val Precision: 0.8591 | Recall: 0.8573 | F1: 0.8579\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.73%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 37/50 [Train]: 100%|██████████| 313/313 [00:18<00:00, 16.60it/s]\nResNet18 - Epoch 37/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 78.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 37 Results:\n   Train Loss: 0.2844 | Train Acc: 89.94%\n   Val Loss: 0.4720 | Val Acc: 85.78%\n   Val Precision: 0.8603 | Recall: 0.8578 | F1: 0.8586\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.78%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 38/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.55it/s]\nResNet18 - Epoch 38/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 74.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 38 Results:\n   Train Loss: 0.2833 | Train Acc: 90.05%\n   Val Loss: 0.4831 | Val Acc: 85.80%\n   Val Precision: 0.8601 | Recall: 0.8580 | F1: 0.8587\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.80%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 39/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.40it/s]\nResNet18 - Epoch 39/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 75.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 39 Results:\n   Train Loss: 0.2750 | Train Acc: 90.25%\n   Val Loss: 0.4805 | Val Acc: 85.70%\n   Val Precision: 0.8582 | Recall: 0.8570 | F1: 0.8574\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 40/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.30it/s]\nResNet18 - Epoch 40/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 71.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 40 Results:\n   Train Loss: 0.2740 | Train Acc: 90.21%\n   Val Loss: 0.4703 | Val Acc: 85.94%\n   Val Precision: 0.8608 | Recall: 0.8594 | F1: 0.8599\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.94%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 41/50 [Train]: 100%|██████████| 313/313 [00:21<00:00, 14.85it/s]\nResNet18 - Epoch 41/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 75.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 41 Results:\n   Train Loss: 0.2689 | Train Acc: 90.44%\n   Val Loss: 0.4863 | Val Acc: 85.72%\n   Val Precision: 0.8594 | Recall: 0.8572 | F1: 0.8578\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 42/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.53it/s]\nResNet18 - Epoch 42/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 74.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 42 Results:\n   Train Loss: 0.2640 | Train Acc: 90.67%\n   Val Loss: 0.4949 | Val Acc: 85.66%\n   Val Precision: 0.8578 | Recall: 0.8566 | F1: 0.8569\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 43/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.37it/s]\nResNet18 - Epoch 43/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 73.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 43 Results:\n   Train Loss: 0.2540 | Train Acc: 90.86%\n   Val Loss: 0.4874 | Val Acc: 85.83%\n   Val Precision: 0.8602 | Recall: 0.8583 | F1: 0.8588\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 44/50 [Train]: 100%|██████████| 313/313 [00:21<00:00, 14.81it/s]\nResNet18 - Epoch 44/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 69.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 44 Results:\n   Train Loss: 0.2570 | Train Acc: 90.91%\n   Val Loss: 0.4875 | Val Acc: 85.66%\n   Val Precision: 0.8586 | Recall: 0.8566 | F1: 0.8573\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 45/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.45it/s]\nResNet18 - Epoch 45/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 74.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 45 Results:\n   Train Loss: 0.2521 | Train Acc: 91.00%\n   Val Loss: 0.4800 | Val Acc: 86.13%\n   Val Precision: 0.8627 | Recall: 0.8613 | F1: 0.8617\n   LR: 0.000100\nNEW BEST MODEL! Val Acc: 86.13%\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 46/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.43it/s]\nResNet18 - Epoch 46/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 74.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 46 Results:\n   Train Loss: 0.2460 | Train Acc: 91.23%\n   Val Loss: 0.4885 | Val Acc: 85.78%\n   Val Precision: 0.8597 | Recall: 0.8578 | F1: 0.8584\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 47/50 [Train]: 100%|██████████| 313/313 [00:21<00:00, 14.64it/s]\nResNet18 - Epoch 47/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 72.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 47 Results:\n   Train Loss: 0.2528 | Train Acc: 90.97%\n   Val Loss: 0.4831 | Val Acc: 86.07%\n   Val Precision: 0.8626 | Recall: 0.8607 | F1: 0.8614\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 48/50 [Train]: 100%|██████████| 313/313 [00:20<00:00, 15.23it/s]\nResNet18 - Epoch 48/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 77.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 48 Results:\n   Train Loss: 0.2442 | Train Acc: 91.18%\n   Val Loss: 0.4926 | Val Acc: 85.82%\n   Val Precision: 0.8601 | Recall: 0.8582 | F1: 0.8589\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 49/50 [Train]: 100%|██████████| 313/313 [00:21<00:00, 14.81it/s]\nResNet18 - Epoch 49/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 72.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 49 Results:\n   Train Loss: 0.2473 | Train Acc: 91.09%\n   Val Loss: 0.4892 | Val Acc: 85.85%\n   Val Precision: 0.8607 | Recall: 0.8585 | F1: 0.8593\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"ResNet18 - Epoch 50/50 [Train]: 100%|██████████| 313/313 [00:21<00:00, 14.79it/s]\nResNet18 - Epoch 50/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 75.17it/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stdout","text":"\nResNet18 - Epoch 50 Results:\n   Train Loss: 0.2447 | Train Acc: 91.47%\n   Val Loss: 0.4817 | Val Acc: 85.96%\n   Val Precision: 0.8609 | Recall: 0.8596 | F1: 0.8600\n   LR: 0.000100\n\nResNet18 Training Complete! Best Val Acc: 86.13%\nResNet18 training history saved to /kaggle/working/model_results/ResNet18\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20.5M/20.5M [00:00<00:00, 130MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Training EfficientNet-B0\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 1/50 [Train]: 100%|██████████| 313/313 [00:25<00:00, 12.46it/s]\nEfficientNet-B0 - Epoch 1/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 52.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 1 Results:\n   Train Loss: 1.4210 | Train Acc: 49.70%\n   Val Loss: 1.3694 | Val Acc: 64.11%\n   Val Precision: 0.6501 | Recall: 0.6411 | F1: 0.6410\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 64.11%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 2/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 12.53it/s]\nEfficientNet-B0 - Epoch 2/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 51.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 2 Results:\n   Train Loss: 0.9949 | Train Acc: 65.82%\n   Val Loss: 0.8339 | Val Acc: 72.55%\n   Val Precision: 0.7294 | Recall: 0.7255 | F1: 0.7240\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 72.55%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 3/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 12.60it/s]\nEfficientNet-B0 - Epoch 3/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 3 Results:\n   Train Loss: 0.8579 | Train Acc: 70.07%\n   Val Loss: 0.6740 | Val Acc: 76.73%\n   Val Precision: 0.7679 | Recall: 0.7673 | F1: 0.7669\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 76.73%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 4/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 12.65it/s]\nEfficientNet-B0 - Epoch 4/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 52.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 4 Results:\n   Train Loss: 0.7810 | Train Acc: 73.14%\n   Val Loss: 0.6193 | Val Acc: 78.52%\n   Val Precision: 0.7913 | Recall: 0.7852 | F1: 0.7856\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 78.52%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 5/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 12.70it/s]\nEfficientNet-B0 - Epoch 5/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 52.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 5 Results:\n   Train Loss: 0.7357 | Train Acc: 74.66%\n   Val Loss: 0.5896 | Val Acc: 79.67%\n   Val Precision: 0.7961 | Recall: 0.7967 | F1: 0.7950\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 79.67%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 6/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 12.60it/s]\nEfficientNet-B0 - Epoch 6/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 6 Results:\n   Train Loss: 0.6911 | Train Acc: 76.16%\n   Val Loss: 0.5305 | Val Acc: 81.55%\n   Val Precision: 0.8203 | Recall: 0.8155 | F1: 0.8157\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 81.55%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 7/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 13.00it/s]\nEfficientNet-B0 - Epoch 7/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 7 Results:\n   Train Loss: 0.6556 | Train Acc: 77.58%\n   Val Loss: 0.5297 | Val Acc: 81.59%\n   Val Precision: 0.8171 | Recall: 0.8159 | F1: 0.8158\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 81.59%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 8/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.20it/s]\nEfficientNet-B0 - Epoch 8/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 57.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 8 Results:\n   Train Loss: 0.6356 | Train Acc: 77.97%\n   Val Loss: 0.5117 | Val Acc: 82.52%\n   Val Precision: 0.8281 | Recall: 0.8252 | F1: 0.8250\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 82.52%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 9/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 12.80it/s]\nEfficientNet-B0 - Epoch 9/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 57.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 9 Results:\n   Train Loss: 0.6112 | Train Acc: 78.78%\n   Val Loss: 0.4930 | Val Acc: 82.89%\n   Val Precision: 0.8320 | Recall: 0.8289 | F1: 0.8289\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 82.89%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 10/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.21it/s]\nEfficientNet-B0 - Epoch 10/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 10 Results:\n   Train Loss: 0.5852 | Train Acc: 79.79%\n   Val Loss: 0.4789 | Val Acc: 83.91%\n   Val Precision: 0.8442 | Recall: 0.8391 | F1: 0.8404\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 83.91%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 11/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 12.84it/s]\nEfficientNet-B0 - Epoch 11/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 53.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 11 Results:\n   Train Loss: 0.5678 | Train Acc: 80.22%\n   Val Loss: 0.4682 | Val Acc: 84.28%\n   Val Precision: 0.8444 | Recall: 0.8428 | F1: 0.8425\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 84.28%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 12/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.12it/s]\nEfficientNet-B0 - Epoch 12/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 57.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 12 Results:\n   Train Loss: 0.5461 | Train Acc: 81.17%\n   Val Loss: 0.4562 | Val Acc: 84.57%\n   Val Precision: 0.8451 | Recall: 0.8457 | F1: 0.8449\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 84.57%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 13/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.11it/s]\nEfficientNet-B0 - Epoch 13/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 13 Results:\n   Train Loss: 0.5372 | Train Acc: 81.37%\n   Val Loss: 0.4546 | Val Acc: 84.67%\n   Val Precision: 0.8469 | Recall: 0.8467 | F1: 0.8462\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 84.67%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 14/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.05it/s]\nEfficientNet-B0 - Epoch 14/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 57.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 14 Results:\n   Train Loss: 0.5193 | Train Acc: 82.14%\n   Val Loss: 0.4517 | Val Acc: 84.71%\n   Val Precision: 0.8490 | Recall: 0.8471 | F1: 0.8476\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 84.71%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 15/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.15it/s]\nEfficientNet-B0 - Epoch 15/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 15 Results:\n   Train Loss: 0.5046 | Train Acc: 82.41%\n   Val Loss: 0.4634 | Val Acc: 84.32%\n   Val Precision: 0.8507 | Recall: 0.8432 | F1: 0.8446\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 16/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.13it/s]\nEfficientNet-B0 - Epoch 16/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 16 Results:\n   Train Loss: 0.4859 | Train Acc: 83.08%\n   Val Loss: 0.4392 | Val Acc: 85.34%\n   Val Precision: 0.8535 | Recall: 0.8534 | F1: 0.8531\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.34%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 17/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.10it/s]\nEfficientNet-B0 - Epoch 17/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 17 Results:\n   Train Loss: 0.4869 | Train Acc: 83.23%\n   Val Loss: 0.4230 | Val Acc: 85.52%\n   Val Precision: 0.8555 | Recall: 0.8552 | F1: 0.8548\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.52%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 18/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.09it/s]\nEfficientNet-B0 - Epoch 18/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 57.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 18 Results:\n   Train Loss: 0.4745 | Train Acc: 83.67%\n   Val Loss: 0.4291 | Val Acc: 85.65%\n   Val Precision: 0.8587 | Recall: 0.8565 | F1: 0.8569\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.65%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 19/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.11it/s]\nEfficientNet-B0 - Epoch 19/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 19 Results:\n   Train Loss: 0.4547 | Train Acc: 84.23%\n   Val Loss: 0.4428 | Val Acc: 84.72%\n   Val Precision: 0.8518 | Recall: 0.8472 | F1: 0.8482\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 20/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.11it/s]\nEfficientNet-B0 - Epoch 20/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 20 Results:\n   Train Loss: 0.4448 | Train Acc: 84.24%\n   Val Loss: 0.4616 | Val Acc: 84.37%\n   Val Precision: 0.8495 | Recall: 0.8437 | F1: 0.8445\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 21/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.21it/s]\nEfficientNet-B0 - Epoch 21/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 21 Results:\n   Train Loss: 0.4354 | Train Acc: 84.69%\n   Val Loss: 0.4343 | Val Acc: 85.05%\n   Val Precision: 0.8508 | Recall: 0.8505 | F1: 0.8499\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 22/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.13it/s]\nEfficientNet-B0 - Epoch 22/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 22 Results:\n   Train Loss: 0.4225 | Train Acc: 85.31%\n   Val Loss: 0.4489 | Val Acc: 85.19%\n   Val Precision: 0.8551 | Recall: 0.8519 | F1: 0.8526\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 23/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.05it/s]\nEfficientNet-B0 - Epoch 23/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 23 Results:\n   Train Loss: 0.4105 | Train Acc: 85.59%\n   Val Loss: 0.4423 | Val Acc: 85.09%\n   Val Precision: 0.8540 | Recall: 0.8509 | F1: 0.8512\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 24/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.18it/s]\nEfficientNet-B0 - Epoch 24/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 24 Results:\n   Train Loss: 0.4153 | Train Acc: 85.53%\n   Val Loss: 0.4450 | Val Acc: 85.66%\n   Val Precision: 0.8635 | Recall: 0.8566 | F1: 0.8585\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.66%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 25/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.12it/s]\nEfficientNet-B0 - Epoch 25/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 57.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 25 Results:\n   Train Loss: 0.4055 | Train Acc: 85.89%\n   Val Loss: 0.4559 | Val Acc: 84.89%\n   Val Precision: 0.8529 | Recall: 0.8489 | F1: 0.8496\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 26/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.06it/s]\nEfficientNet-B0 - Epoch 26/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 53.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 26 Results:\n   Train Loss: 0.3921 | Train Acc: 86.47%\n   Val Loss: 0.4242 | Val Acc: 85.76%\n   Val Precision: 0.8594 | Recall: 0.8576 | F1: 0.8577\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.76%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 27/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.10it/s]\nEfficientNet-B0 - Epoch 27/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 27 Results:\n   Train Loss: 0.3805 | Train Acc: 86.68%\n   Val Loss: 0.4194 | Val Acc: 85.89%\n   Val Precision: 0.8607 | Recall: 0.8589 | F1: 0.8590\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 85.89%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 28/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 13.04it/s]\nEfficientNet-B0 - Epoch 28/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 28 Results:\n   Train Loss: 0.3788 | Train Acc: 86.89%\n   Val Loss: 0.4118 | Val Acc: 86.13%\n   Val Precision: 0.8620 | Recall: 0.8613 | F1: 0.8613\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 86.13%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 29/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.13it/s]\nEfficientNet-B0 - Epoch 29/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 29 Results:\n   Train Loss: 0.3630 | Train Acc: 87.19%\n   Val Loss: 0.4164 | Val Acc: 86.04%\n   Val Precision: 0.8634 | Recall: 0.8604 | F1: 0.8612\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 30/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 13.01it/s]\nEfficientNet-B0 - Epoch 30/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 30 Results:\n   Train Loss: 0.3588 | Train Acc: 87.57%\n   Val Loss: 0.4203 | Val Acc: 86.15%\n   Val Precision: 0.8640 | Recall: 0.8615 | F1: 0.8618\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 86.15%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 31/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.16it/s]\nEfficientNet-B0 - Epoch 31/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 31 Results:\n   Train Loss: 0.3530 | Train Acc: 87.70%\n   Val Loss: 0.4236 | Val Acc: 86.48%\n   Val Precision: 0.8684 | Recall: 0.8648 | F1: 0.8656\n   LR: 0.001000\nNEW BEST MODEL! Val Acc: 86.48%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 32/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.07it/s]\nEfficientNet-B0 - Epoch 32/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 32 Results:\n   Train Loss: 0.3412 | Train Acc: 88.19%\n   Val Loss: 0.4310 | Val Acc: 85.85%\n   Val Precision: 0.8605 | Recall: 0.8585 | F1: 0.8590\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 33/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.11it/s]\nEfficientNet-B0 - Epoch 33/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 33 Results:\n   Train Loss: 0.3353 | Train Acc: 88.13%\n   Val Loss: 0.4511 | Val Acc: 85.91%\n   Val Precision: 0.8616 | Recall: 0.8591 | F1: 0.8592\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 34/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.04it/s]\nEfficientNet-B0 - Epoch 34/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 34 Results:\n   Train Loss: 0.3311 | Train Acc: 88.46%\n   Val Loss: 0.4397 | Val Acc: 86.15%\n   Val Precision: 0.8641 | Recall: 0.8615 | F1: 0.8621\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 35/50 [Train]: 100%|██████████| 313/313 [00:24<00:00, 13.00it/s]\nEfficientNet-B0 - Epoch 35/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 57.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 35 Results:\n   Train Loss: 0.3343 | Train Acc: 88.38%\n   Val Loss: 0.4397 | Val Acc: 85.58%\n   Val Precision: 0.8590 | Recall: 0.8558 | F1: 0.8567\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 36/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.04it/s]\nEfficientNet-B0 - Epoch 36/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 36 Results:\n   Train Loss: 0.3165 | Train Acc: 89.02%\n   Val Loss: 0.4409 | Val Acc: 86.13%\n   Val Precision: 0.8636 | Recall: 0.8613 | F1: 0.8618\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 37/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.10it/s]\nEfficientNet-B0 - Epoch 37/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 52.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 37 Results:\n   Train Loss: 0.3058 | Train Acc: 89.32%\n   Val Loss: 0.4420 | Val Acc: 86.22%\n   Val Precision: 0.8644 | Recall: 0.8622 | F1: 0.8629\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 38/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.15it/s]\nEfficientNet-B0 - Epoch 38/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 38 Results:\n   Train Loss: 0.2988 | Train Acc: 89.44%\n   Val Loss: 0.4498 | Val Acc: 86.31%\n   Val Precision: 0.8659 | Recall: 0.8631 | F1: 0.8638\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 39/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.18it/s]\nEfficientNet-B0 - Epoch 39/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 39 Results:\n   Train Loss: 0.2979 | Train Acc: 89.53%\n   Val Loss: 0.4339 | Val Acc: 86.23%\n   Val Precision: 0.8627 | Recall: 0.8623 | F1: 0.8619\n   LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 40/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.15it/s]\nEfficientNet-B0 - Epoch 40/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 40 Results:\n   Train Loss: 0.2313 | Train Acc: 91.94%\n   Val Loss: 0.4102 | Val Acc: 87.40%\n   Val Precision: 0.8761 | Recall: 0.8740 | F1: 0.8746\n   LR: 0.000100\nNEW BEST MODEL! Val Acc: 87.40%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 41/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.19it/s]\nEfficientNet-B0 - Epoch 41/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 41 Results:\n   Train Loss: 0.2038 | Train Acc: 92.81%\n   Val Loss: 0.4199 | Val Acc: 87.43%\n   Val Precision: 0.8760 | Recall: 0.8743 | F1: 0.8748\n   LR: 0.000100\nNEW BEST MODEL! Val Acc: 87.43%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 42/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.13it/s]\nEfficientNet-B0 - Epoch 42/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 42 Results:\n   Train Loss: 0.1960 | Train Acc: 93.13%\n   Val Loss: 0.4164 | Val Acc: 87.74%\n   Val Precision: 0.8794 | Recall: 0.8774 | F1: 0.8780\n   LR: 0.000100\nNEW BEST MODEL! Val Acc: 87.74%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 43/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.16it/s]\nEfficientNet-B0 - Epoch 43/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 43 Results:\n   Train Loss: 0.1916 | Train Acc: 93.29%\n   Val Loss: 0.4192 | Val Acc: 87.82%\n   Val Precision: 0.8800 | Recall: 0.8782 | F1: 0.8787\n   LR: 0.000100\nNEW BEST MODEL! Val Acc: 87.82%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 44/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.09it/s]\nEfficientNet-B0 - Epoch 44/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 44 Results:\n   Train Loss: 0.1823 | Train Acc: 93.66%\n   Val Loss: 0.4239 | Val Acc: 87.69%\n   Val Precision: 0.8789 | Recall: 0.8769 | F1: 0.8774\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 45/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.11it/s]\nEfficientNet-B0 - Epoch 45/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 54.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 45 Results:\n   Train Loss: 0.1769 | Train Acc: 93.69%\n   Val Loss: 0.4245 | Val Acc: 87.93%\n   Val Precision: 0.8808 | Recall: 0.8793 | F1: 0.8797\n   LR: 0.000100\nNEW BEST MODEL! Val Acc: 87.93%\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 46/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.14it/s]\nEfficientNet-B0 - Epoch 46/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 46 Results:\n   Train Loss: 0.1798 | Train Acc: 93.66%\n   Val Loss: 0.4225 | Val Acc: 87.84%\n   Val Precision: 0.8812 | Recall: 0.8784 | F1: 0.8793\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 47/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.23it/s]\nEfficientNet-B0 - Epoch 47/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 47 Results:\n   Train Loss: 0.1660 | Train Acc: 94.20%\n   Val Loss: 0.4305 | Val Acc: 87.76%\n   Val Precision: 0.8789 | Recall: 0.8776 | F1: 0.8779\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 48/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.18it/s]\nEfficientNet-B0 - Epoch 48/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 48 Results:\n   Train Loss: 0.1651 | Train Acc: 94.20%\n   Val Loss: 0.4376 | Val Acc: 87.56%\n   Val Precision: 0.8776 | Recall: 0.8756 | F1: 0.8763\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 49/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.21it/s]\nEfficientNet-B0 - Epoch 49/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 55.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 49 Results:\n   Train Loss: 0.1629 | Train Acc: 94.21%\n   Val Loss: 0.4328 | Val Acc: 87.88%\n   Val Precision: 0.8794 | Recall: 0.8788 | F1: 0.8789\n   LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 - Epoch 50/50 [Train]: 100%|██████████| 313/313 [00:23<00:00, 13.21it/s]\nEfficientNet-B0 - Epoch 50/50 [Val]: 100%|██████████| 79/79 [00:01<00:00, 56.41it/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"\nEfficientNet-B0 - Epoch 50 Results:\n   Train Loss: 0.1535 | Train Acc: 94.58%\n   Val Loss: 0.4397 | Val Acc: 87.85%\n   Val Precision: 0.8802 | Recall: 0.8785 | F1: 0.8790\n   LR: 0.000100\n\nEfficientNet-B0 Training Complete! Best Val Acc: 87.93%\nEfficientNet-B0 training history saved to /kaggle/working/model_results/EfficientNet-B0\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 205MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Training VGG16\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 1/50 [Train]: 100%|██████████| 313/313 [00:39<00:00,  7.89it/s]\nVGG16 - Epoch 1/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 1 Results:\n   Train Loss: 1.7083 | Train Acc: 33.91%\n   Val Loss: 1.1534 | Val Acc: 58.45%\n   Val Precision: 0.5956 | Recall: 0.5845 | F1: 0.5589\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 58.45%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 2/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s]\nVGG16 - Epoch 2/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 2 Results:\n   Train Loss: 1.0643 | Train Acc: 63.88%\n   Val Loss: 0.8259 | Val Acc: 72.63%\n   Val Precision: 0.7555 | Recall: 0.7263 | F1: 0.7211\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 72.63%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 3/50 [Train]: 100%|██████████| 313/313 [00:39<00:00,  8.01it/s]\nVGG16 - Epoch 3/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 3 Results:\n   Train Loss: 0.8270 | Train Acc: 72.86%\n   Val Loss: 0.6773 | Val Acc: 79.19%\n   Val Precision: 0.8035 | Recall: 0.7919 | F1: 0.7931\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 79.19%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 4/50 [Train]: 100%|██████████| 313/313 [00:39<00:00,  8.00it/s]\nVGG16 - Epoch 4/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 4 Results:\n   Train Loss: 0.7179 | Train Acc: 76.79%\n   Val Loss: 0.5667 | Val Acc: 81.83%\n   Val Precision: 0.8257 | Recall: 0.8183 | F1: 0.8175\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 81.83%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 5/50 [Train]: 100%|██████████| 313/313 [00:39<00:00,  8.01it/s]\nVGG16 - Epoch 5/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 5 Results:\n   Train Loss: 0.6546 | Train Acc: 78.88%\n   Val Loss: 0.5241 | Val Acc: 83.04%\n   Val Precision: 0.8380 | Recall: 0.8304 | F1: 0.8292\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 83.04%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 6/50 [Train]: 100%|██████████| 313/313 [00:39<00:00,  8.01it/s]\nVGG16 - Epoch 6/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 6 Results:\n   Train Loss: 0.6184 | Train Acc: 79.97%\n   Val Loss: 0.4686 | Val Acc: 84.41%\n   Val Precision: 0.8465 | Recall: 0.8441 | F1: 0.8441\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 84.41%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 7/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.06it/s]\nVGG16 - Epoch 7/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 7 Results:\n   Train Loss: 0.5647 | Train Acc: 81.99%\n   Val Loss: 0.5000 | Val Acc: 84.47%\n   Val Precision: 0.8556 | Recall: 0.8447 | F1: 0.8446\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 84.47%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 8/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.05it/s]\nVGG16 - Epoch 8/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 8 Results:\n   Train Loss: 0.5472 | Train Acc: 82.58%\n   Val Loss: 0.4717 | Val Acc: 85.29%\n   Val Precision: 0.8583 | Recall: 0.8529 | F1: 0.8532\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 85.29%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 9/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.08it/s]\nVGG16 - Epoch 9/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 9 Results:\n   Train Loss: 0.5239 | Train Acc: 83.42%\n   Val Loss: 0.4177 | Val Acc: 86.69%\n   Val Precision: 0.8695 | Recall: 0.8669 | F1: 0.8673\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 86.69%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 10/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.05it/s]\nVGG16 - Epoch 10/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 10 Results:\n   Train Loss: 0.4926 | Train Acc: 84.12%\n   Val Loss: 0.4293 | Val Acc: 86.19%\n   Val Precision: 0.8679 | Recall: 0.8619 | F1: 0.8625\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 11/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.11it/s]\nVGG16 - Epoch 11/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 11 Results:\n   Train Loss: 0.4773 | Train Acc: 84.81%\n   Val Loss: 0.4213 | Val Acc: 86.85%\n   Val Precision: 0.8700 | Recall: 0.8685 | F1: 0.8673\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 86.85%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 12/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.06it/s]\nVGG16 - Epoch 12/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 12 Results:\n   Train Loss: 0.4672 | Train Acc: 85.16%\n   Val Loss: 0.3914 | Val Acc: 87.76%\n   Val Precision: 0.8792 | Recall: 0.8776 | F1: 0.8777\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 87.76%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 13/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.09it/s]\nVGG16 - Epoch 13/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 13 Results:\n   Train Loss: 0.4445 | Train Acc: 85.79%\n   Val Loss: 0.3870 | Val Acc: 87.99%\n   Val Precision: 0.8831 | Recall: 0.8799 | F1: 0.8804\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 87.99%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 14/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.06it/s]\nVGG16 - Epoch 14/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 14 Results:\n   Train Loss: 0.4413 | Train Acc: 86.25%\n   Val Loss: 0.4162 | Val Acc: 87.11%\n   Val Precision: 0.8770 | Recall: 0.8711 | F1: 0.8723\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 15/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 15/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 15 Results:\n   Train Loss: 0.4250 | Train Acc: 86.55%\n   Val Loss: 0.4096 | Val Acc: 87.48%\n   Val Precision: 0.8811 | Recall: 0.8748 | F1: 0.8753\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 16/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.15it/s]\nVGG16 - Epoch 16/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 16 Results:\n   Train Loss: 0.4023 | Train Acc: 87.14%\n   Val Loss: 0.4294 | Val Acc: 86.78%\n   Val Precision: 0.8742 | Recall: 0.8678 | F1: 0.8689\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 17/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.10it/s]\nVGG16 - Epoch 17/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 17 Results:\n   Train Loss: 0.3905 | Train Acc: 87.48%\n   Val Loss: 0.3741 | Val Acc: 88.42%\n   Val Precision: 0.8873 | Recall: 0.8842 | F1: 0.8846\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 88.42%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 18/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.09it/s]\nVGG16 - Epoch 18/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 18 Results:\n   Train Loss: 0.3967 | Train Acc: 87.47%\n   Val Loss: 0.3956 | Val Acc: 87.55%\n   Val Precision: 0.8809 | Recall: 0.8755 | F1: 0.8737\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 19/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s]\nVGG16 - Epoch 19/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 19 Results:\n   Train Loss: 0.3796 | Train Acc: 88.00%\n   Val Loss: 0.3637 | Val Acc: 88.70%\n   Val Precision: 0.8878 | Recall: 0.8870 | F1: 0.8863\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 88.70%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 20/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.06it/s]\nVGG16 - Epoch 20/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 20 Results:\n   Train Loss: 0.3844 | Train Acc: 87.91%\n   Val Loss: 0.4027 | Val Acc: 88.36%\n   Val Precision: 0.8862 | Recall: 0.8836 | F1: 0.8840\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 21/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 21/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 21 Results:\n   Train Loss: 0.3817 | Train Acc: 88.03%\n   Val Loss: 0.3759 | Val Acc: 88.33%\n   Val Precision: 0.8872 | Recall: 0.8833 | F1: 0.8832\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 22/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 22/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 22 Results:\n   Train Loss: 0.3608 | Train Acc: 88.47%\n   Val Loss: 0.4285 | Val Acc: 87.40%\n   Val Precision: 0.8768 | Recall: 0.8740 | F1: 0.8730\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 23/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s]\nVGG16 - Epoch 23/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 23 Results:\n   Train Loss: 0.3592 | Train Acc: 88.75%\n   Val Loss: 0.3777 | Val Acc: 88.70%\n   Val Precision: 0.8906 | Recall: 0.8870 | F1: 0.8874\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 24/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s]\nVGG16 - Epoch 24/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 24 Results:\n   Train Loss: 0.3484 | Train Acc: 88.96%\n   Val Loss: 0.3654 | Val Acc: 88.65%\n   Val Precision: 0.8889 | Recall: 0.8865 | F1: 0.8864\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 25/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.14it/s]\nVGG16 - Epoch 25/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 25 Results:\n   Train Loss: 0.3395 | Train Acc: 89.47%\n   Val Loss: 0.3619 | Val Acc: 89.20%\n   Val Precision: 0.8934 | Recall: 0.8920 | F1: 0.8921\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 89.20%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 26/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.09it/s]\nVGG16 - Epoch 26/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 29.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 26 Results:\n   Train Loss: 0.3309 | Train Acc: 89.83%\n   Val Loss: 0.4071 | Val Acc: 88.47%\n   Val Precision: 0.8887 | Recall: 0.8847 | F1: 0.8850\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 27/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s]\nVGG16 - Epoch 27/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 27 Results:\n   Train Loss: 0.3256 | Train Acc: 89.86%\n   Val Loss: 0.3641 | Val Acc: 89.21%\n   Val Precision: 0.8944 | Recall: 0.8921 | F1: 0.8926\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 89.21%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 28/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.08it/s]\nVGG16 - Epoch 28/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 28 Results:\n   Train Loss: 0.3194 | Train Acc: 90.05%\n   Val Loss: 0.4464 | Val Acc: 88.14%\n   Val Precision: 0.8834 | Recall: 0.8814 | F1: 0.8809\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 29/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 29/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 29 Results:\n   Train Loss: 0.3268 | Train Acc: 89.80%\n   Val Loss: 0.3714 | Val Acc: 88.21%\n   Val Precision: 0.8842 | Recall: 0.8821 | F1: 0.8817\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 30/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.18it/s]\nVGG16 - Epoch 30/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 30 Results:\n   Train Loss: 0.3215 | Train Acc: 90.19%\n   Val Loss: 0.3823 | Val Acc: 89.43%\n   Val Precision: 0.8976 | Recall: 0.8943 | F1: 0.8946\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 89.43%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 31/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.10it/s]\nVGG16 - Epoch 31/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 31 Results:\n   Train Loss: 0.2984 | Train Acc: 90.55%\n   Val Loss: 0.3624 | Val Acc: 89.43%\n   Val Precision: 0.8999 | Recall: 0.8943 | F1: 0.8956\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 32/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s]\nVGG16 - Epoch 32/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 32 Results:\n   Train Loss: 0.3025 | Train Acc: 90.52%\n   Val Loss: 0.3597 | Val Acc: 88.95%\n   Val Precision: 0.8900 | Recall: 0.8895 | F1: 0.8886\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 33/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 33/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 33 Results:\n   Train Loss: 0.3013 | Train Acc: 90.51%\n   Val Loss: 0.3733 | Val Acc: 88.74%\n   Val Precision: 0.8881 | Recall: 0.8874 | F1: 0.8864\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 34/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 34/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 34 Results:\n   Train Loss: 0.3021 | Train Acc: 90.66%\n   Val Loss: 0.3590 | Val Acc: 89.49%\n   Val Precision: 0.8956 | Recall: 0.8949 | F1: 0.8948\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 89.49%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 35/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.10it/s]\nVGG16 - Epoch 35/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 35 Results:\n   Train Loss: 0.2923 | Train Acc: 90.80%\n   Val Loss: 0.3817 | Val Acc: 89.25%\n   Val Precision: 0.8942 | Recall: 0.8925 | F1: 0.8926\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 36/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.18it/s]\nVGG16 - Epoch 36/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 36 Results:\n   Train Loss: 0.2838 | Train Acc: 91.03%\n   Val Loss: 0.3431 | Val Acc: 89.15%\n   Val Precision: 0.8920 | Recall: 0.8915 | F1: 0.8914\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 37/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 37/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 37 Results:\n   Train Loss: 0.2810 | Train Acc: 91.34%\n   Val Loss: 0.3641 | Val Acc: 88.93%\n   Val Precision: 0.8912 | Recall: 0.8893 | F1: 0.8892\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 38/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 38/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 38 Results:\n   Train Loss: 0.2771 | Train Acc: 91.51%\n   Val Loss: 0.3829 | Val Acc: 89.31%\n   Val Precision: 0.8947 | Recall: 0.8931 | F1: 0.8926\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 39/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 39/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 39 Results:\n   Train Loss: 0.2814 | Train Acc: 91.27%\n   Val Loss: 0.3676 | Val Acc: 89.06%\n   Val Precision: 0.8944 | Recall: 0.8906 | F1: 0.8908\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 40/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 40/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 40 Results:\n   Train Loss: 0.2790 | Train Acc: 91.36%\n   Val Loss: 0.3487 | Val Acc: 89.93%\n   Val Precision: 0.9006 | Recall: 0.8993 | F1: 0.8996\n   LR: 0.000500\nNEW BEST MODEL! Val Acc: 89.93%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 41/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.13it/s]\nVGG16 - Epoch 41/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 41 Results:\n   Train Loss: 0.2666 | Train Acc: 91.61%\n   Val Loss: 0.3790 | Val Acc: 89.06%\n   Val Precision: 0.8946 | Recall: 0.8906 | F1: 0.8909\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 42/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 42/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 42 Results:\n   Train Loss: 0.2616 | Train Acc: 91.91%\n   Val Loss: 0.3510 | Val Acc: 89.79%\n   Val Precision: 0.8989 | Recall: 0.8979 | F1: 0.8977\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 43/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.19it/s]\nVGG16 - Epoch 43/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 43 Results:\n   Train Loss: 0.2750 | Train Acc: 91.52%\n   Val Loss: 0.3695 | Val Acc: 89.46%\n   Val Precision: 0.8973 | Recall: 0.8946 | F1: 0.8953\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 44/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.18it/s]\nVGG16 - Epoch 44/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 44 Results:\n   Train Loss: 0.2576 | Train Acc: 92.00%\n   Val Loss: 0.3496 | Val Acc: 89.75%\n   Val Precision: 0.8996 | Recall: 0.8975 | F1: 0.8978\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 45/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.18it/s]\nVGG16 - Epoch 45/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 45 Results:\n   Train Loss: 0.2720 | Train Acc: 91.70%\n   Val Loss: 0.4150 | Val Acc: 88.25%\n   Val Precision: 0.8864 | Recall: 0.8825 | F1: 0.8831\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 46/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.19it/s]\nVGG16 - Epoch 46/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 46 Results:\n   Train Loss: 0.2793 | Train Acc: 91.35%\n   Val Loss: 0.4505 | Val Acc: 86.83%\n   Val Precision: 0.8763 | Recall: 0.8683 | F1: 0.8681\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 47/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.18it/s]\nVGG16 - Epoch 47/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 47 Results:\n   Train Loss: 0.2579 | Train Acc: 92.11%\n   Val Loss: 0.4300 | Val Acc: 89.13%\n   Val Precision: 0.8958 | Recall: 0.8913 | F1: 0.8925\n   LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 48/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.18it/s]\nVGG16 - Epoch 48/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 48 Results:\n   Train Loss: 0.1784 | Train Acc: 94.45%\n   Val Loss: 0.3333 | Val Acc: 90.54%\n   Val Precision: 0.9064 | Recall: 0.9054 | F1: 0.9057\n   LR: 0.000050\nNEW BEST MODEL! Val Acc: 90.54%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 49/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.15it/s]\nVGG16 - Epoch 49/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 49 Results:\n   Train Loss: 0.1476 | Train Acc: 95.26%\n   Val Loss: 0.3362 | Val Acc: 90.80%\n   Val Precision: 0.9086 | Recall: 0.9080 | F1: 0.9082\n   LR: 0.000050\nNEW BEST MODEL! Val Acc: 90.80%\n","output_type":"stream"},{"name":"stderr","text":"VGG16 - Epoch 50/50 [Train]: 100%|██████████| 313/313 [00:38<00:00,  8.17it/s]\nVGG16 - Epoch 50/50 [Val]: 100%|██████████| 79/79 [00:02<00:00, 30.86it/s]","output_type":"stream"},{"name":"stdout","text":"\nVGG16 - Epoch 50 Results:\n   Train Loss: 0.1472 | Train Acc: 95.28%\n   Val Loss: 0.3244 | Val Acc: 90.80%\n   Val Precision: 0.9084 | Recall: 0.9080 | F1: 0.9081\n   LR: 0.000050\n\nVGG16 Training Complete! Best Val Acc: 90.80%\nVGG16 training history saved to /kaggle/working/model_results/VGG16\nTRAINING COMPLETE - SUMMARY\nResNet18:\n   Best Validation Accuracy: 86.13%\n   Best F1 Score: 0.8617\n   Results saved to: /kaggle/working/model_results/ResNet18/\n\nEfficientNet-B0:\n   Best Validation Accuracy: 87.93%\n   Best F1 Score: 0.8797\n   Results saved to: /kaggle/working/model_results/EfficientNet-B0/\n\nVGG16:\n   Best Validation Accuracy: 90.80%\n   Best F1 Score: 0.9082\n   Results saved to: /kaggle/working/model_results/VGG16/\n\nAll model results saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# KAGGLE gave a freeze error so i am gonna run the rest of the code in other NOTEBOOK","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}