{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a454c97",
   "metadata": {},
   "source": [
    "Now we are gonna be getting the similairty for FAKE VS REAL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364746c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "data_folder = r\"D:\\ML League\\Intelligence-Sig-Recs-2025\\NonMandatoryTasks\\SemanticDatasetComparison\\train\"\n",
    "\n",
    "real = os.path.join(data_folder, 'REAL')\n",
    "fake = os.path.join(data_folder, 'FAKE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b543ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_images(folder):\n",
    "    exts = ('.png','.jpg','.jpeg','.bmp')\n",
    "    return sorted([os.path.join(folder,f) for f in os.listdir(folder) if f.lower().endswith(exts)])\n",
    "\n",
    "real_img = list_images(real)\n",
    "fake_img = list_images(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c32ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b58a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\",use_fast = 'True')\n",
    "\n",
    "def get_clip_embeddings(files):\n",
    "    all_feats = []\n",
    "    clip_model.eval()\n",
    "    for i in tqdm(range(0, len(files), BATCH_SIZE), desc=f\"Processing {len(files)} images\"):\n",
    "        batch_files = files[i:i+BATCH_SIZE]\n",
    "        imgs = [Image.open(f).convert(\"RGB\") for f in batch_files]\n",
    "        inputs = clip_processor(images=imgs, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            feats = clip_model.get_image_features(**inputs)\n",
    "        all_feats.append(feats.cpu())\n",
    "    return torch.cat(all_feats, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0165c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fake_emb = get_clip_embeddings(fake_img)\\nreal_emb = get_clip_embeddings(real_img)\\n\\ntorch.save(fake_emb, 'fake_clip_embeddings.pt')\\ntorch.save(real_emb, 'real_clip_embeddings.pt')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''fake_emb = get_clip_embeddings(fake_img)\n",
    "real_emb = get_clip_embeddings(real_img)\n",
    "\n",
    "torch.save(fake_emb, 'fake_clip_embeddings.pt')\n",
    "torch.save(real_emb, 'real_clip_embeddings.pt')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb185f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_emb = torch.load('fake_clip_embeddings.pt')\n",
    "real_emb = torch.load('real_clip_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940b05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "real_norm = F.normalize(real_emb, dim=1)\n",
    "fake_norm = F.normalize(fake_emb, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f4114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid Cosine Similarity (REAL vs FAKE): 0.9913\n"
     ]
    }
   ],
   "source": [
    "c_real = real_norm.mean(dim=0, keepdim=True)\n",
    "c_fake = fake_norm.mean(dim=0, keepdim=True)\n",
    "\n",
    "centroid_cos = F.cosine_similarity(c_real, c_fake).item()\n",
    "print(f\"Centroid Cosine Similarity (REAL vs FAKE): {centroid_cos:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07e31c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Pairwise Cosine Similarity: 0.7882\n"
     ]
    }
   ],
   "source": [
    "N = min(len(real_norm), len(fake_norm), 1000)\n",
    "sample_real = real_norm[:N]\n",
    "sample_fake = fake_norm[:N]\n",
    "\n",
    "avg_pairwise_cos = (sample_real @ sample_fake.T).mean().item()\n",
    "print(f\"Average Pairwise Cosine Similarity: {avg_pairwise_cos:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4821fe",
   "metadata": {},
   "source": [
    "The centroid similarity shows that the FAKE dataset captures the overall semantic distribution of REAL images almost perfectly. However, the lower average pairwise similarity might be due to the unavialability of the labels and mismatch similairty matching between different labels which led to a lesser overall score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59cba88",
   "metadata": {},
   "source": [
    "So i think we can try Frechet Inception distance as it gives a more statsitical comparison between those images and we can get a better idea of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf59608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "inception = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1,\n",
    "                                aux_logits=True, transform_input=False)\n",
    "inception.fc = nn.Identity()\n",
    "inception.to(device).eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Inception input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b55bf92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [30:37<00:00,  1.18s/it]\n",
      "100%|██████████| 1563/1563 [28:49<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings extracted: (50000, 2048) (50000, 2048)\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    paths = [os.path.join(folder, f) for f in os.listdir(folder) \n",
    "             if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    return paths\n",
    "\n",
    "def get_inception_embeddings_gpu(image_paths, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        imgs = []\n",
    "        for p in batch_paths:\n",
    "            img = Image.open(p).convert('RGB')\n",
    "            img = preprocess(img)\n",
    "            imgs.append(img)\n",
    "        imgs = torch.stack(imgs).to(device)  # push batch to GPU\n",
    "        with torch.no_grad():\n",
    "            feat = inception(imgs)  # [B, 2048] on GPU\n",
    "        embeddings.append(feat.cpu().numpy())  # move to CPU for saving\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "real_folder = r\"D:\\ML League\\Intelligence-Sig-Recs-2025\\NonMandatoryTasks\\SemanticDatasetComparison\\train\\REAL\"\n",
    "fake_folder = r\"D:\\ML League\\Intelligence-Sig-Recs-2025\\NonMandatoryTasks\\SemanticDatasetComparison\\train\\FAKE\"\n",
    "\n",
    "real_paths = load_images_from_folder(real_folder)\n",
    "fake_paths = load_images_from_folder(fake_folder)\n",
    "\n",
    "real_emb = get_inception_embeddings_gpu(real_paths)\n",
    "fake_emb = get_inception_embeddings_gpu(fake_paths)\n",
    "\n",
    "# Save embeddings\n",
    "np.save('real_inception_embeddings.npy', real_emb)\n",
    "np.save('fake_inception_embeddings.npy', fake_emb)\n",
    "\n",
    "print(\"Embeddings extracted:\", real_emb.shape, fake_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7adef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nithi\\AppData\\Local\\Temp\\ipykernel_24716\\1496668021.py:15: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
      "  covmean, _ = sqrtm(sigma_real.dot(sigma_fake), disp=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID (REAL vs FAKE): 26.0692\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(real_embeddings: np.ndarray, fake_embeddings: np.ndarray):\n",
    "    mu_real = np.mean(real_embeddings, axis=0)\n",
    "    mu_fake = np.mean(fake_embeddings, axis=0)\n",
    "    sigma_real = np.cov(real_embeddings, rowvar=False)\n",
    "    sigma_fake = np.cov(fake_embeddings, rowvar=False)\n",
    "\n",
    "    diff = mu_real - mu_fake\n",
    "    diff_squared = diff.dot(diff)\n",
    "\n",
    "    covmean, _ = sqrtm(sigma_real.dot(sigma_fake), disp=False)\n",
    "\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid_value = diff_squared + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid_value\n",
    "\n",
    "real_emb = np.load(\"real_inception_embeddings.npy\")\n",
    "fake_emb = np.load(\"fake_inception_embeddings.npy\")\n",
    "\n",
    "fid_score = calculate_fid(real_emb, fake_emb)\n",
    "print(f\"FID (REAL vs FAKE): {fid_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3327865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [04:22<00:00,  1.19it/s]\n",
      "100%|██████████| 313/313 [04:20<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "real_folder = r\"D:\\ML League\\Intelligence-Sig-Recs-2025\\NonMandatoryTasks\\SemanticDatasetComparison\\test\\REAL\"\n",
    "fake_folder = r\"D:\\ML League\\Intelligence-Sig-Recs-2025\\NonMandatoryTasks\\SemanticDatasetComparison\\test\\FAKE\"\n",
    "\n",
    "real_paths_test = load_images_from_folder(real_folder)\n",
    "fake_paths_test = load_images_from_folder(fake_folder)\n",
    "\n",
    "real_emb_test = get_inception_embeddings_gpu(real_paths_test)\n",
    "fake_emb_test = get_inception_embeddings_gpu(fake_paths_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a60f2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('real_inception_embeddings_test.npy', real_emb_test)\n",
    "np.save('fake_inception_embeddings_test.npy', fake_emb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5832cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled real embeddings: (10000, 2048)\n",
      "Sampled fake embeddings: (10000, 2048)\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10000  # number of random samples\n",
    "\n",
    "# Make sure we don't sample more than available\n",
    "num_samples_real = min(num_samples, real_emb.shape[0])\n",
    "num_samples_fake = min(num_samples, fake_emb.shape[0])\n",
    "\n",
    "# Random indices\n",
    "real_indices = np.random.choice(real_emb.shape[0], num_samples_real, replace=False)\n",
    "fake_indices = np.random.choice(fake_emb.shape[0], num_samples_fake, replace=False)\n",
    "\n",
    "# Sampled embeddings\n",
    "real_sampled = real_emb[real_indices]\n",
    "fake_sampled = fake_emb[fake_indices]\n",
    "\n",
    "print(\"Sampled real embeddings:\", real_sampled.shape)\n",
    "print(\"Sampled fake embeddings:\", fake_sampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf45fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nithi\\AppData\\Local\\Temp\\ipykernel_24716\\1496668021.py:15: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
      "  covmean, _ = sqrtm(sigma_real.dot(sigma_fake), disp=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID TEST (REAL vs FAKE): 30.0561\n",
      "FID (REAL vs REAL_TEST): 4.7516\n",
      "FID (FAKE vs FAKE_TEST): 4.6602\n"
     ]
    }
   ],
   "source": [
    "fid_score = calculate_fid(real_emb_test, fake_emb_test)\n",
    "print(f\"FID TEST (REAL vs FAKE): {fid_score:.4f}\")\n",
    "\n",
    "# Just to see how it comes out on test data\n",
    "\n",
    "fid_score = calculate_fid(real_sampled, real_emb_test)\n",
    "print(f\"FID (REAL vs REAL_TEST): {fid_score:.4f}\")\n",
    "\n",
    "fid_score = calculate_fid(fake_sampled, fake_emb_test)\n",
    "print(f\"FID (FAKE vs FAKE_TEST): {fid_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e0b8d",
   "metadata": {},
   "source": [
    "Statistically confirms that distribution of fake embeddings differs from real embeddings. These calculations of FID between classes was just to showcase the how the FID Score is given out for semantically similar class.If a class is perfectly similar semantci similarity is close to 0.So even though the FAKE dataset may have reached a very good similiarity with the REAL Dataset there are some imperfections or flaws in them which make them different from REAL dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_League",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
